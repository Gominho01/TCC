{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42b3be0a",
   "metadata": {},
   "source": [
    "# **IMPORTS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "302cf303",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "from torchvision.models import VGG16_Weights, VGG19_Weights\n",
    "from torchvision.models import ResNet50_Weights, ResNet152_Weights\n",
    "from torchvision.models import DenseNet201_Weights\n",
    "from torchvision.models import Inception_V3_Weights\n",
    "from torchvision.models import MobileNet_V2_Weights\n",
    "import torchvision\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, log_loss\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torchvision import transforms\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0fcdce35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando o dispositivo: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Usando o dispositivo: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49171b95",
   "metadata": {},
   "source": [
    "# **Transformando Imagens**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3fd54362",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess_224 = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "preprocess_299 = transforms.Compose([\n",
    "    transforms.Resize(299),\n",
    "    transforms.CenterCrop(299),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20380791",
   "metadata": {},
   "source": [
    "# **Doenças**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "035d519c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\n",
    "    'Atelectasis', 'Consolidation', 'Infiltration', 'Pneumothorax', 'Edema',\n",
    "    'Emphysema', 'Fibrosis', 'Effusion', 'Pneumonia', 'Pleural_Thickening',\n",
    "    'Cardiomegaly', 'Nodule', 'Hernia', 'Mass', 'No Finding'\n",
    "]\n",
    "num_classes = len(class_names)\n",
    "\n",
    "def one_hot_encode_labels(labels_list):\n",
    "    encoder = OneHotEncoder(categories=[range(num_classes)], sparse_output=False)\n",
    "    labels = encoder.fit_transform(np.array(labels_list).reshape(-1, 1))\n",
    "    return torch.tensor(labels, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18ec9fe",
   "metadata": {},
   "source": [
    "# Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "993fe304",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, log_loss\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_model_multilabel(model, val_loader, device, num_classes):\n",
    "    \"\"\"\n",
    "    Avalia um modelo multilabel em um conjunto de validação usando várias métricas.\n",
    "\n",
    "    Parâmetros:\n",
    "    - model: O modelo a ser avaliado.\n",
    "    - val_loader: DataLoader para o conjunto de validação.\n",
    "    - device: O dispositivo (CPU/GPU) onde o modelo está.\n",
    "    - num_classes: Número de classes para o problema multilabel.\n",
    "\n",
    "    Retorna:\n",
    "    - Um dicionário contendo as métricas calculadas.\n",
    "    \"\"\"\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "    outputs_flat = []\n",
    "\n",
    "    # Loop de validação\n",
    "    model.eval()  # Coloca o modelo em modo de avaliação\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            predicted = (outputs > 0.5).float()\n",
    "            \n",
    "            # Guardar as previsões e saídas reais\n",
    "            y_true.extend(labels.cpu().numpy())\n",
    "            y_pred.extend(predicted.cpu().numpy())\n",
    "            outputs_flat.extend(outputs.cpu().numpy())\n",
    "\n",
    "    # Converter as saídas e rótulos para binário para cada classe\n",
    "    y_true = np.array(y_true)\n",
    "    y_pred = np.array(y_pred)\n",
    "\n",
    "    # Calcula as métricas para cada classe\n",
    "    precision = precision_score(y_true, y_pred, average='macro')\n",
    "    recall = recall_score(y_true, y_pred, average='macro')\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    roc_auc = roc_auc_score(y_true, outputs_flat, average='macro')\n",
    "    log_loss_value = log_loss(y_true, outputs_flat)\n",
    "\n",
    "    # Matriz de confusão para cada classe\n",
    "    for i in range(num_classes):\n",
    "        conf_matrix = confusion_matrix(y_true[:, i], y_pred[:, i])\n",
    "        plt.figure(figsize=(6, 4))\n",
    "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "        plt.ylabel('True Labels')\n",
    "        plt.xlabel('Predicted Labels')\n",
    "        plt.title(f'Confusion Matrix - Class {i}')\n",
    "        plt.show()\n",
    "\n",
    "    # Imprimir as métricas\n",
    "    print(f'Precision: {precision}')\n",
    "    print(f'Recall: {recall}')\n",
    "    print(f'F1-Score: {f1}')\n",
    "    print(f'ROC AUC: {roc_auc}')\n",
    "    print(f'Log Loss: {log_loss_value}')\n",
    "\n",
    "    # Retornar as métricas como um dicionário\n",
    "    return {\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'roc_auc': roc_auc,\n",
    "        'log_loss': log_loss_value\n",
    "    }\n",
    "\n",
    "# Exemplo de como chamar a função para avaliar o modelo multilabel:\n",
    "# metrics = evaluate_model_multilabel(vgg16, val_loader, device, num_classes=num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291f4e7d",
   "metadata": {},
   "source": [
    "# Treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cc8a2db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, optimizer, criterion, num_epochs, device):\n",
    "    loss_history = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        progress_bar = tqdm(train_loader, desc=f'Época {epoch+1}/{num_epochs}', unit='batch')\n",
    "\n",
    "        for images, labels in progress_bar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        loss_history.append(epoch_loss)\n",
    "        print(f'Época {epoch+1}, Loss: {epoch_loss}')\n",
    "\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f03309",
   "metadata": {},
   "source": [
    "# **Classe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c6ef389",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChestXrayDataset(Dataset):\n",
    "    def __init__(self, image_list, dataframe, img_dir, transform=None):\n",
    "        self.image_list = image_list\n",
    "        self.dataframe = dataframe\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "        self.encoder = OneHotEncoder(categories=[range(num_classes)], sparse_output=False)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.image_list[idx]\n",
    "        img_path = os.path.join(self.img_dir, 'images', img_name)\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        labels_str = self.dataframe.loc[self.dataframe['Image Index'] == img_name, 'Finding Labels'].values[0]\n",
    "        labels_list = labels_str.split('|')\n",
    "\n",
    "        # Convert labels to indices\n",
    "        label_indices = [class_names.index(label) for label in labels_list]\n",
    "\n",
    "        # One-hot encode the labels\n",
    "        labels = torch.zeros(num_classes)\n",
    "        if label_indices:\n",
    "            labels = self.encoder.fit_transform(np.array(label_indices).reshape(-1, 1))\n",
    "            labels = torch.tensor(labels.sum(axis=0), dtype=torch.float32)\n",
    "\n",
    "        return image, labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5275c0",
   "metadata": {},
   "source": [
    "## Variaveis Globais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d78945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = 'D:/Users/Lucas/Downloads/Outra Pasta/'\n",
    "metadata_df = pd.read_csv(base_path + 'Data_Entry_2017.csv')\n",
    "\n",
    "def load_image_list(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        image_list = file.read().splitlines()\n",
    "    return image_list\n",
    "\n",
    "train_val_list = load_image_list(base_path + 'train_val_list.txt')\n",
    "test_list = load_image_list(base_path + 'test_list.txt')\n",
    "num_epochs = 5\n",
    "num_classes = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb0a0c25",
   "metadata": {},
   "source": [
    "# **Modelos**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0859a8af",
   "metadata": {},
   "source": [
    "## **VGG16**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "965ee747",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = models.vgg16(weights=VGG16_Weights.IMAGENET1K_V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "690492a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16 = vgg16.to(device)\n",
    "\n",
    "vgg16.classifier[6] = nn.Linear(vgg16.classifier[6].in_features, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3145562b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, val_list = train_test_split(train_val_list, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = ChestXrayDataset(train_list, metadata_df, base_path, transform=preprocess_224)\n",
    "val_dataset = ChestXrayDataset(val_list, metadata_df, base_path, transform=preprocess_224)\n",
    "test_dataset = ChestXrayDataset(test_list, metadata_df, base_path, transform=preprocess_224)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6561f4e",
   "metadata": {},
   "source": [
    "### **Treino**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0ff564e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 1/5: 100%|██████████| 2164/2164 [1:20:15<00:00,  2.23s/batch, loss=0.233]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1, Loss: 0.20315680467625202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 2/5: 100%|██████████| 2164/2164 [1:15:29<00:00,  2.09s/batch, loss=0.237]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 2, Loss: 0.19829703822332587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 3/5: 100%|██████████| 2164/2164 [1:15:32<00:00,  2.09s/batch, loss=0.23] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 3, Loss: 0.19820399107945164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 4/5: 100%|██████████| 2164/2164 [1:16:02<00:00,  2.11s/batch, loss=0.5]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 4, Loss: 0.1982504340291519\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 5/5: 100%|██████████| 2164/2164 [1:16:00<00:00,  2.11s/batch, loss=0.158]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 5, Loss: 0.19808177654106385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(vgg16.parameters(), lr=0.001)\n",
    "\n",
    "loss_history = train_model(vgg16, train_loader, optimizer, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a083c0cd",
   "metadata": {},
   "source": [
    "### **Avaliação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "42988e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg16.eval()\n",
    "\n",
    "metrics = evaluate_model_multilabel(vgg16, val_loader, device, num_classes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86c3292",
   "metadata": {},
   "source": [
    "## **VGG19**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825a94c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to C:\\Users\\Windows/.cache\\torch\\hub\\checkpoints\\vgg19-dcbb9e9d.pth\n",
      "100%|██████████| 548M/548M [00:05<00:00, 107MB/s]  \n"
     ]
    }
   ],
   "source": [
    "vgg19 = models.vgg19(weights=VGG19_Weights.IMAGENET1K_V1)\n",
    "\n",
    "vgg19 = vgg19.to(device)\n",
    "\n",
    "vgg19.classifier[6] = nn.Linear(vgg19.classifier[6].in_features, num_classes).to(device)\n",
    "\n",
    "train_list, val_list = train_test_split(train_val_list, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = ChestXrayDataset(train_list, metadata_df, base_path, transform=preprocess_224)\n",
    "val_dataset = ChestXrayDataset(val_list, metadata_df, base_path, transform=preprocess_224)\n",
    "test_dataset = ChestXrayDataset(test_list, metadata_df, base_path, transform=preprocess_224)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407bca4b",
   "metadata": {},
   "source": [
    "### **Treino**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c5eb0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 1/5: 100%|██████████| 2164/2164 [1:28:46<00:00,  2.46s/batch, loss=0.128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1, Loss: 0.24808471013280578\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 2/5: 100%|██████████| 2164/2164 [1:22:09<00:00,  2.28s/batch, loss=0.0884]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 2, Loss: 0.19818850679670816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 3/5: 100%|██████████| 2164/2164 [1:21:49<00:00,  2.27s/batch, loss=0.254]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 3, Loss: 0.19815622946305989\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 4/5: 100%|██████████| 2164/2164 [1:23:12<00:00,  2.31s/batch, loss=0.332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 4, Loss: 0.1981703033997865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 5/5: 100%|██████████| 2164/2164 [1:26:04<00:00,  2.39s/batch, loss=0.199]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 5, Loss: 0.19803778880900685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(vgg19.parameters(), lr=0.001)\n",
    "\n",
    "loss_history = train_model(vgg19, train_loader, optimizer, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b059385",
   "metadata": {},
   "source": [
    "### **Avaliação**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6025f1a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validação Loss: 0.19994776994816257, Acurácia: 91.9113936241934%\n"
     ]
    }
   ],
   "source": [
    "vgg19.eval()\n",
    "\n",
    "metrics = evaluate_model_multilabel(vgg19, val_loader, device, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092cfd05",
   "metadata": {},
   "source": [
    "## **Inception-V3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9fa608",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_v3 = models.inception_v3(weights=models.Inception_V3_Weights.IMAGENET1K_V1)\n",
    "inception_v3 = inception_v3.to(device)\n",
    "\n",
    "inception_v3.fc = nn.Linear(inception_v3.fc.in_features, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b143f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_list, val_list = train_test_split(train_val_list, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = ChestXrayDataset(train_list, metadata_df, base_path, transform=preprocess_299)\n",
    "val_dataset = ChestXrayDataset(val_list, metadata_df, base_path, transform=preprocess_299)\n",
    "test_dataset = ChestXrayDataset(test_list, metadata_df, base_path, transform=preprocess_299)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "inception_v3 = models.inception_v3(weights=Inception_V3_Weights.IMAGENET1K_V1, aux_logits=True)\n",
    "inception_v3 = inception_v3.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9578bb95",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Época 1/5:   0%|          | 0/2164 [00:46<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 14\u001b[0m\n\u001b[0;32m     10\u001b[0m images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     12\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m outputs, _ \u001b[38;5;241m=\u001b[39m \u001b[43minception_v3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     17\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\inception.py:166\u001b[0m, in \u001b[0;36mInception3.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m InceptionOutputs:\n\u001b[0;32m    165\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transform_input(x)\n\u001b[1;32m--> 166\u001b[0m     x, aux \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    167\u001b[0m     aux_defined \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maux_logits\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mis_scripting():\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\inception.py:113\u001b[0m, in \u001b[0;36mInception3._forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    111\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool1(x)\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# N x 64 x 73 x 73\u001b[39;00m\n\u001b[1;32m--> 113\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv2d_3b_1x1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;66;03m# N x 80 x 73 x 73\u001b[39;00m\n\u001b[0;32m    115\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mConv2d_4a_3x3(x)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torchvision\\models\\inception.py:407\u001b[0m, in \u001b[0;36mBasicConv2d.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    405\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[0;32m    406\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn(x)\n\u001b[1;32m--> 407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\nn\\functional.py:1498\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1496\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(relu, (\u001b[38;5;28minput\u001b[39m,), \u001b[38;5;28minput\u001b[39m, inplace\u001b[38;5;241m=\u001b[39minplace)\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m-> 1498\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1500\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28minput\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    inception_v3.train()\n",
    "    running_loss = 0.0\n",
    "\n",
    "    progress_bar = tqdm(train_loader, desc=f'Época {epoch+1}/{num_epochs}', unit='batch')\n",
    "\n",
    "    for images, labels in progress_bar:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, _ = inception_v3(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f'Época {epoch+1}, Loss: {running_loss/len(train_loader)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498be7ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "inception_v3.eval()\n",
    "metrics = evaluate_model_multilabel(inception_v3, val_loader, device, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97ccc1b",
   "metadata": {},
   "source": [
    "# **ResNet50**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a225568",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to C:\\Users\\Windows/.cache\\torch\\hub\\checkpoints\\resnet50-0676ba61.pth\n",
      "100%|██████████| 97.8M/97.8M [00:01<00:00, 81.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "resnet50 = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V1)\n",
    "resnet50 = resnet50.to(device)\n",
    "\n",
    "resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804e3178",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = ChestXrayDataset(train_list, metadata_df, base_path, transform=preprocess_224)\n",
    "val_dataset = ChestXrayDataset(val_list, metadata_df, base_path, transform=preprocess_224)\n",
    "test_dataset = ChestXrayDataset(test_list, metadata_df, base_path, transform=preprocess_224)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7278ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(resnet50.parameters(), lr=0.001)\n",
    "\n",
    "loss_history = train_model(resnet50, train_loader, optimizer, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a029404",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet50.eval()\n",
    "metrics = evaluate_model_multilabel(resnet50, val_loader, device, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fda5600",
   "metadata": {},
   "source": [
    " # **ResNet152**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac24ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet152 = models.resnet152(weights=models.ResNet152_Weights.IMAGENET1K_V1)\n",
    "resnet152 = resnet152.to(device)\n",
    "\n",
    "resnet152.fc = nn.Linear(resnet152.fc.in_features, num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99e2377",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(resnet152.parameters(), lr=0.001)\n",
    "\n",
    "train_dataset = ChestXrayDataset(train_list, metadata_df, base_path, transform=preprocess_224)\n",
    "val_dataset = ChestXrayDataset(val_list, metadata_df, base_path, transform=preprocess_224)\n",
    "test_dataset = ChestXrayDataset(test_list, metadata_df, base_path, transform=preprocess_224)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a765f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_history = train_model(resnet152, train_loader, optimizer, criterion, num_epochs, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1996674",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet152.eval()\n",
    "metrics = evaluate_model_multilabel(resnet152, val_loader, device, num_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
